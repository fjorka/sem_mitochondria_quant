"""
Processes raw axon masks generated by SAM from tiled predictions.
It extracts region properties, filters objects based on morphology and overlap,
identifies axon interiors (soma), and saves the final results to a DataFrame.
"""

import argparse
import os
import sys
import time
import pickle as pkl
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import dask.array as da
import numpy as np
import pandas as pd
from loguru import logger
from tqdm import tqdm

# --- Import shared components ---
try:
    from sem_quant.utils import setup_logging, smart_path
    from sem_quant.load_config import load_config, PipelineConfig
    from sem_quant.data_utils import load_image_dask
    from sem_quant.processing_utils import (
        extract_regionprops,
        suppress_by_iou,
        add_soma_data,
        mark_tile_edge_objects,
        mask_from_df # Kept for potential future use/consistency
    )
    UTILS_AVAILABLE = True
except ImportError as e:
    print(f"Error: Could not import necessary functions. "
            f"Please ensure 'utils.py', 'load_config.py', 'data_utils.py', "
            f"and 'processing_utils.py' are accessible in a 'sem_quant' package or Python path. "
            f"Error: {e}")
    UTILS_AVAILABLE = False
    # Exit if essential utilities are missing
    sys.exit(1)

# --- Constants ---
LOGS_SUBDIR = 'logs'
ANNOTATION_SUFFIX = ".pkl"
TILE_MASK_PREFIX = 'tile_masks_' # Prefix for input SAM mask files per tile
# Region properties to extract for the myelin sheath (outer ring)
MYELIN_PROPS = [
    'area', 'area_convex', 'area_filled', 'euler_number', 'image', 'bbox',
    'eccentricity', 'solidity', 'centroid', 'major_axis_length',
    'minor_axis_length'
]
# Region properties to extract for the soma (inner axon)
SOMA_PROPS = [
    'area', 'eccentricity', 'major_axis_length', 'minor_axis_length',
    'centroid', 'bbox', 'image'
]

# --- Function Definitions ---

def process_single_tile(
    tile_file_path: Path,
    myelin_props: List[str],
    small_object_size: int
) -> Optional[pd.DataFrame]:
    """
    Loads raw masks from a single tile pickle file, extracts region properties
    for individual objects within each raw mask, and merges results.

    Args:
        tile_file_path: Path to the input pickle file for the tile.
        myelin_props: List of properties to extract using regionprops.
        small_object_size: Minimum size for morphological cleaning in extract_regionprops.

    Returns:
        A DataFrame containing processed object data for the tile, or None if processing fails.
    """
    logger.debug(f"Processing {tile_file_path.name}")
    try:
        # Load raw masks for this tile
        with open(tile_file_path, 'rb') as f:
            df_tile = pd.read_pickle(f)
        logger.debug(f"Loaded {len(df_tile)} raw masks from {tile_file_path.name}.")

        if df_tile.empty:
             logger.info(f"Tile file {tile_file_path.name} is empty. Skipping.")
             return None

        # Check for required input columns from SAM segmentation script
        if not all(col in df_tile.columns for col in ['bbox', 'segmentation']):
             logger.error(f"Missing required columns ('bbox', 'segmentation') in {tile_file_path.name}. Skipping.")
             return None

        # Divide raw masks into individual objects and get properties
        props_list_tile = []
        for ind, row in df_tile.iterrows():
            try:
                # Extract properties for each sub-component within the raw mask
                props = extract_regionprops(row, myelin_props, small_size=small_object_size)
                if not props.empty:
                    props['origin_sam_index'] = ind # Keep track of the original raw mask index
                    props_list_tile.append(props)
                else:
                     logger.trace(f"No valid regions found for raw mask index {ind} in {tile_file_path.name} after extraction.")
            except Exception as e_rp:
                 logger.warning(f"Could not extract regionprops for raw mask index {ind} in {tile_file_path.name}. Error: {e_rp}")
                 continue # Skip this mask component

        if not props_list_tile:
            logger.warning(f"No valid objects extracted from {tile_file_path.name}. Skipping tile.")
            return None

        props_all_tile = pd.concat(props_list_tile, ignore_index=True)
        # Rename 'area' to avoid conflict with potential SAM 'area'
        props_all_tile.rename(columns={'area': 'axon_area'}, inplace=True) # rp_area = regionprops area

        # Merge with original tile metadata (like tile coords)
        # Drop large segmentation data, keep other metadata from SAM output
        cols_to_drop = ['segmentation']
        df_tile_meta = df_tile.drop(columns=[col for col in cols_to_drop if col in df_tile.columns]).copy()
        df_tile_meta['origin_sam_index'] = df_tile_meta.index # Add index for merging

        # Merge regionprops results with the metadata from the original SAM mask row
        df_merged = pd.merge(df_tile_meta, props_all_tile, on='origin_sam_index', how='right')

        if df_merged.empty:
             logger.warning(f"Merge failed between SAM mask metadata and regionprops for {tile_file_path.name}.")
             return None

        # Note: Global label assignment will happen after concatenating all tiles
        logger.debug(f"Successfully processed {len(df_merged)} potential axons from {tile_file_path.name}.")
        return df_merged

    except FileNotFoundError:
        logger.error(f"Tile file not found: {tile_file_path}")
        return None
    except Exception as e:
        logger.error(f"Failed to process {tile_file_path.name}: {e}", exc_info=True)
        return None


def run_axon_processing(config: PipelineConfig):
    """
    Main processing pipeline for axon masks.

    Args:
        config: Loaded pipeline configuration object.
    """
    start_time = time.time()
    logger.info("--- Starting Axon Mask Processing ---")

    # --- Setup Paths & Parameters from Config ---
    paths = config.paths
    data_props = config.data_properties
    axons_filters = config.axons_filters

    analysis_dir = Path(smart_path(paths.analysis_dir))
    axons_dir = analysis_dir / paths.axons_data_suffix
    im_path_str = paths.im_path
    output_prefix = paths.output_prefix
    axons_data_suffix = paths.axons_data_suffix


    # --- Load Image Metadata (for Shape) ---
    # Only shape is needed here, not the full dask array for processing
    try:
        # Use utility function to get shape (image not needed)
        _, im_shape = load_image_dask(im_path_str, data_props.axons_res)
        logger.info(f"Successfully retrieved image shape: {im_shape} for resolution {data_props.axons_res}")
    except (FileNotFoundError, KeyError, Exception) as e:
        logger.error(f"Could not load image metadata from {im_path_str} at resolution {data_props.axons_res}: {e}")
        return 

    # --- Find and Process Tile Mask Files ---
    tile_mask_files = sorted([
        f for f in axons_dir.glob(f'{TILE_MASK_PREFIX}*{ANNOTATION_SUFFIX}')
        if f.is_file()
    ])

    if not tile_mask_files:
        logger.error(f"No '{TILE_MASK_PREFIX}*{ANNOTATION_SUFFIX}' files found in {analysis_dir}. Cannot proceed.")
        return

    logger.info(f"Found {len(tile_mask_files)} tile mask files to process.")

    # Process each tile file
    all_props_list = []
    processed_tiles = 0
    skipped_tiles = 0
    for tile_file in tqdm(tile_mask_files, desc="Processing Tile Masks"):
        df_tile_processed = process_single_tile(tile_file, MYELIN_PROPS, config.axons_filters.small_object_area_threshold)
        if df_tile_processed is not None and not df_tile_processed.empty:
            all_props_list.append(df_tile_processed)
            processed_tiles += 1
        else:
            skipped_tiles += 1

    logger.info(f"Finished processing loop. Tiles processed: {processed_tiles}, Tiles skipped: {skipped_tiles}")

    if not all_props_list:
        logger.error("No objects were successfully processed from any tile file. Exiting.")
        return

    # --- Combine and Filter All Objects ---
    try:
        logger.info("Concatenating results from all tiles...")
        df_combined = pd.concat(all_props_list, ignore_index=True)
        # Assign unique global labels now
        df_combined['label'] = df_combined.index + 1
        logger.info(f"Combined {len(df_combined)} objects from all processed tiles.")
    except Exception as e:
        logger.error(f"Error during final concatenation: {e}", exc_info=True)
        return

    # --- Adjust Coordinates to Full Image Space ---
    coord_cols_to_adjust = {
        'bbox-0': 'tile_row_start', 
        'bbox-1': 'tile_col_start',
        'bbox-2': 'tile_row_start', 
        'bbox-3': 'tile_col_start',
        'centroid-0': 'tile_row_start', 
        'centroid-1': 'tile_col_start'
    }
    missing_cols = [col for col in list(coord_cols_to_adjust.keys()) + list(coord_cols_to_adjust.values()) if col not in df_combined.columns]
    if missing_cols:
         logger.error(f"Missing columns required for coordinate adjustment: {missing_cols}. Check DataFrame structure after tile processing.")
         return

    try:
        for obj_col, tile_offset_col in coord_cols_to_adjust.items():
             # Ensure columns are numeric before addition
             df_combined[obj_col] = pd.to_numeric(df_combined[obj_col], errors='coerce')
             df_combined[tile_offset_col] = pd.to_numeric(df_combined[tile_offset_col], errors='coerce')
             df_combined[obj_col] = df_combined[obj_col] + df_combined[tile_offset_col]
        logger.info("Adjusted object coordinates to global image space.")
    except Exception as e:
         logger.error(f"Error during coordinate adjustment: {e}", exc_info=True)
         return

    # --- Filter based on morphological properties ---
    initial_count = len(df_combined)
    solidity_threshold = axons_filters.max_solidity
    # Note: The notebook uses rp_area / area_filled. Check properties are extracted.
    filter_cols = ['euler_number', 'axon_area', 'area_filled']
    missing_filter_cols = [col for col in filter_cols if col not in df_combined.columns]
    if missing_filter_cols:
         logger.error(f"Missing columns required for morphological filtering: {missing_filter_cols}. Check MYELIN_PROPS.")
         return

    # Calculate df_combined['rp_area'] / df_combined['area_filled'] for filtering
    df_combined['solidity_metric'] = np.divide(df_combined['axon_area'], df_combined['area_filled'], out=np.zeros_like(df_combined['axon_area'], dtype=float), where=df_combined['area_filled']!=0)


    # Perform filtering using boolean indexing with .loc for safety
    try:
        df_filtered = df_combined.loc[
            (df_combined['euler_number'] < 1) & # Ring-like structures
            (df_combined['solidity_metric'] < solidity_threshold) # Less solid == thinner ring
        ].copy() # Use copy to avoid SettingWithCopyWarning
        logger.info(f"Filtered by Euler number (<1) and solidity metric (<{solidity_threshold}): {initial_count} -> {len(df_filtered)} objects.")
    except KeyError as e:
         logger.error(f"Error accessing column during filtering: {e}. Columns available: {df_combined.columns.tolist()}")
         return
    except Exception as e:
        logger.error(f"An unexpected error occurred during morphological filtering: {e}", exc_info=True)
        return


    # --- Filter objects touching internal tile boundaries ---
    # overlap between the tiles should be sufficient for every object to be fully contained in at least one tile
    # `mark_tile_edge_objects` requires tile end coordinates as well
    tile_end_cols = ['tile_row_end', 'tile_col_end']
    if not all(col in df_filtered.columns for col in tile_end_cols):
         logger.warning(f"Tile end coordinates {tile_end_cols} not found. Skipping internal tile edge object filtering.")
         df_filtered['edge_ring'] = False # Assume no edge rings if can't filter
    else:
        try:
            # mark_tile_edge_objects needs bbox-0,1,2,3 and tile_row/col_start/end
            df_filtered = mark_tile_edge_objects(df_filtered, pad=config.axons_filters.tile_edge_padding, filter='not_edge')
            # Initialize column 'edge_ring' needed for add_soma_data later.
            # It will be properly set inside add_soma_data if an object IS an edge ring.
            df_filtered['edge_ring'] = False
            logger.info(f"Removed internal tile edge objects (padding={config.axons_filters.tile_edge_padding}): {len(df_filtered)} remaining.")
        except Exception as e:
            logger.error(f"Error during tile edge object filtering: {e}", exc_info=True)
            logger.warning("Proceeding without internal tile edge filtering due to error.")
            df_filtered['edge_ring'] = False # Ensure column exists


    # --- Suppress highly overlapping objects using IoU ---
    df_filtered = df_filtered.reset_index(drop=True) # Ensure unique index for suppression
    iou_threshold = axons_filters.iou_threshold
    # suppress_by_iou requires 'bbox-0' ... 'bbox-3', 'solidity', 'euler_number'
    suppress_cols = ['bbox-0', 'bbox-1', 'bbox-2', 'bbox-3', 'solidity', 'euler_number']
    missing_suppress_cols = [col for col in suppress_cols if col not in df_filtered.columns]
    if missing_suppress_cols:
        logger.error(f"Missing columns required for IoU suppression: {missing_suppress_cols}. Check MYELIN_PROPS.")
        return

    try:
        df_suppressed = suppress_by_iou(df_filtered, iou_threshold=iou_threshold)
        df_final = df_suppressed.loc[df_suppressed['keep'] == 1].copy() # Keep only non-suppressed items
        logger.info(f"Suppressed overlapping objects (IoU > {iou_threshold}): {len(df_filtered)} -> {len(df_final)} objects.")
    except Exception as e:
         logger.error(f"Error during IoU suppression: {e}", exc_info=True)
         return

    if df_final.empty:
        logger.warning("No objects remaining after IoU suppression. No output will be generated.")
        return

    # --- Add Soma Data ---
    # `add_soma_data` requires 'image', 'bbox-0'...'bbox-3', 'edge_ring'
    # It also needs 'edge_side' if edge_ring is True, but add_soma_data handles edge logic internally.
    soma_cols = ['image', 'bbox-0', 'bbox-1', 'bbox-2', 'bbox-3', 'edge_ring']
    missing_soma_cols = [col for col in soma_cols if col not in df_final.columns]
    if missing_soma_cols:
        logger.error(f"Missing columns required for add_soma_data: {missing_soma_cols}. Check MYELIN_PROPS and previous steps.")
        return

    try:
        logger.info("Starting add_soma_data step...")
        df_final = add_soma_data(df_final.copy(), SOMA_PROPS, pad = config.axons_filters.soma_edge_padding) # Pass copy
        logger.info("Finished add_soma_data step.")
    except Exception as e:
         logger.error(f"Failed during add_soma_data: {e}", exc_info=True)
         return

    # --- Final Cleanup and Labeling ---
    df_final = df_final.reset_index(drop=True)
    # Re-assign final labels after all filtering
    df_final['label'] = df_final.index + 1
    # Add 'inside_label' based on final 'label' if soma extraction was successful
    if 'inside_image' in df_final.columns:
        df_final['inside_label'] = df_final['label']
    else:
        logger.warning("Column 'inside_image' not found after add_soma_data. 'inside_label' cannot be assigned.")


    logger.info(f"Final processed axon count: {len(df_final)}")

    # --- Save Final DataFrame ---
    output_filename = f"{output_prefix}{axons_data_suffix}_test_{ANNOTATION_SUFFIX}"
    output_path = analysis_dir / output_filename
    try:
        # Ensure necessary columns like 'image' and 'inside_image' (if present) contain serializable data (numpy arrays)
        for col in ['image', 'inside_image']:
             if col in df_final.columns:
                  # Ensure all entries are numpy arrays or None
                  df_final[col] = df_final[col].apply(lambda x: np.array(x) if x is not None and not isinstance(x, np.ndarray) else x)

        df_final.to_pickle(output_path)
        logger.info(f"Successfully saved final axon DataFrame to: {output_path}")
    except Exception as e:
        logger.error(f"Failed to save final DataFrame to {output_path}: {e}", exc_info=True)

    end_time = time.time()
    logger.info(f"Total execution time: {end_time - start_time:.2f} seconds")
    logger.info("--- Axon Mask Processing Script Finished ---")


# --- Main Execution ---
if __name__ == "__main__":
    
    # --- Argument Parsing ---
    parser = argparse.ArgumentParser(description="Process axons...")
    parser.add_argument("config_path", type=str, help="Path to config JSON.")
    args = parser.parse_args()

    # --- Load Config ---
    try:
        print(f"Loading configuration from: {args.config_path}")
        config = load_config(args.config_path)
    except Exception as e:
        print(f"FATAL: Failed to load configuration '{args.config_path}': {e}")
        exit(1)

    # --- Setup Logging using Config ---
    try:
        analysis_dir = Path(smart_path(config.paths.analysis_dir))
        log_dir = analysis_dir / LOGS_SUBDIR
        log_file_path = log_dir / f"process_axons_{{time}}.log"
    except Exception as e:
         print(f"WARNING: Error determining log path from config: {e}.")
         exit(1)

    setup_logging(log_file_path=log_file_path)

    # --- Run Main Logic ---
    try:
        run_axon_processing(config) # Pass the loaded config object
    except Exception as e:
        logger.exception("Pipeline step failed with an unhandled exception.")

    # --- Successful Exit ---
    sys.exit(0)
